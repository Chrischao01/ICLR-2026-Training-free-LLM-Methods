# ICLR-2026-Training-free-LLM-Methods
I am currently reproducing all training-free methods submitted to ICLR 2026. 

Current Paper List

COLD-Steer: Steering Large Language Models via In-Context One-step Learning Dynamics
- Review Info: Rating: [4 6 6 8] | Confidence: [3 2 3 3]

In-Context Prompt Optimisation for Knowledge Editing: Enhancing Safety and Coherency in Large Language Models
- Review Info: Rating: [2 2 2 4 4] | Confidence: [3 4 3 3 3]

Decoupled Alignment for Robust Plug-and-Play Adaptation
- Review Info: Rating: [2 4 4 6] | Confidence: [4 3 4 2]

ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training
- Review Info: Rating: [4 4 6 6] | Confidence: [4 3 3 5]

Toward Preference-aligned Large Language Models via Residual-based Model Steering
- Review Info: Rating: [2 2 4] | Confidence: [4 4 4]

Fast Intent Classification for LLM Routing via Statistical Analysis of Representations
- Review Info: Rating: [2 2 4 8] | Confidence: [3 4 2 4] 

Command-V: Training-Free Representation Finetuning Transfer
- Review Info: Rating: [6 6 6 10] | Confidence: [3 3 4 5]

Training-Free Group Relative Policy Optimization
- Review Info: Rating: [2 2 4 6] | Confidence: [4 4 5 3]

SinkTrack: Attention Sink based Context Anchoring for Large Language Models
- Review Info: Rating: [4 4 4 6 6] | Confidence: [3 2 4 3 3] 

Plug-and-Play Global Memory via Test-Time Registers
- Review Info: Rating: [2 4 4 4] | Confidence: [4 3 2 3]

